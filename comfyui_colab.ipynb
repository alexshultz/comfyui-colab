{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexshultz/comfyui-colab/blob/main/comfyui_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaaaaaaaaa"
      },
      "source": [
        "Git clone the repo and install the requirements. (ignore the pip errors about protobuf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Connect Google Drive\n",
        "\n",
        "\n",
        "OPTIONS = {}\n",
        "\n",
        "USE_GOOGLE_DRIVE = True  #@param {type:\"boolean\"}\n",
        "WORKSPACE = 'ComfyUI'\n",
        "OPTIONS['USE_GOOGLE_DRIVE'] = USE_GOOGLE_DRIVE\n",
        "\n",
        "if OPTIONS['USE_GOOGLE_DRIVE']:\n",
        "    !echo \"Mounting Google Drive...\"\n",
        "    %cd /\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    WORKSPACE = \"/content/drive/MyDrive/ComfyUI\"\n",
        "    %cd /content/drive/MyDrive\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xa9geVRnbyOw",
        "outputId": "8ab9d3a6-9d87-4546-9ae3-cf6641341bf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "/\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbbbbbbbbb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Environment Setup\n",
        "\n",
        "\n",
        "OPTIONS = {}\n",
        "\n",
        "USE_GOOGLE_DRIVE = True  #@param {type:\"boolean\"}\n",
        "UPDATE_COMFY_UI = True  #@param {type:\"boolean\"}\n",
        "WORKSPACE = 'ComfyUI'\n",
        "OPTIONS['USE_GOOGLE_DRIVE'] = USE_GOOGLE_DRIVE\n",
        "OPTIONS['UPDATE_COMFY_UI'] = UPDATE_COMFY_UI\n",
        "\n",
        "if OPTIONS['USE_GOOGLE_DRIVE']:\n",
        "    !echo \"Mounting Google Drive...\"\n",
        "    %cd /\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    WORKSPACE = \"/content/drive/MyDrive/ComfyUI\"\n",
        "    %cd /content/drive/MyDrive\n",
        "\n",
        "![ ! -d $WORKSPACE ] && echo -= Initial setup ComfyUI =- && git clone https://github.com/comfyanonymous/ComfyUI\n",
        "%cd $WORKSPACE\n",
        "\n",
        "if OPTIONS['UPDATE_COMFY_UI']:\n",
        "  !echo -= Updating ComfyUI =-\n",
        "  !git pull\n",
        "\n",
        "!echo -= Install dependencies =-\n",
        "# Install xFormers for CUDA 12.4 first\n",
        "!pip install xformers!=0.0.18 --index-url https://download.pytorch.org/whl/cu124\n",
        "# Then install remaining requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# InsightFace and onnxruntime\n",
        "!echo -= Installing InsightFace and onnxruntime for IP-Adapter FaceID =-\n",
        "!pip install insightface onnxruntime-gpu\n",
        "\n",
        "# Update or install IP-Adapter custom nodes\n",
        "!echo -= Updating ComfyUI_IPAdapter_plus =-\n",
        "# If directory exists and is a git repo, update it; otherwise, clone fresh\n",
        "!cd /content/drive/MyDrive/ComfyUI/custom_nodes && (cd ComfyUI_IPAdapter_plus && git pull || git clone https://github.com/cubiq/ComfyUI_IPAdapter_plus.git ComfyUI_IPAdapter_plus)\n",
        "# Install dependencies\n",
        "!if [ -f /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI_IPAdapter_plus/requirements.txt ]; then pip install -r /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI_IPAdapter_plus/requirements.txt; else echo \"requirements.txt missing, installing manually\" && pip install insightface opencv-python-headless numpy torch torchvision; fi\n",
        "\n",
        "# Update or install ComfyUI_essentials custom nodes\n",
        "!echo -= Updating ComfyUI_essentials =-\n",
        "# If directory exists and is a git repo, update it; otherwise, clone fresh\n",
        "!cd /content/drive/MyDrive/ComfyUI/custom_nodes && (cd ComfyUI_essentials && git pull || git clone https://github.com/cubiq/ComfyUI_essentials.git ComfyUI_essentials)\n",
        "# Install dependencies\n",
        "!if [ -f /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI_essentials/requirements.txt ]; then pip install -r /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI_essentials/requirements.txt; else echo \"No additional requirements for ComfyUI_essentials\"; fi\n",
        "\n",
        "\n",
        "# Update or install Comfy_Dungeon custom nodes\n",
        "# https://github.com/cubiq/Comfy_Dungeon\n",
        "!echo -= Updating Comfy_Dungeon =-\n",
        "!cd /content/drive/MyDrive/ComfyUI/custom_nodes && (cd Comfy_Dungeon && git pull || git clone https://github.com/cubiq/Comfy_Dungeon.git Comfy_Dungeon)\n",
        "# No requirements.txt specified in README, assuming base ComfyUI deps suffice\n",
        "\n",
        "# Install huggingface model dowloader\n",
        "!echo -= huggingface_hub =-\n",
        "!pip install huggingface_hub\n",
        "\n",
        "\n",
        "!echo -= Done =-\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cccccccccc"
      },
      "source": [
        "Download some models/checkpoints/vae or custom comfyui nodes (uncomment the commands for the ones you want)\n",
        "\n",
        "Great tools in the AP-Adapter package\n",
        "`https://github.com/cubiq/ComfyUI_IPAdapter_plus?tab=readme-ov-file`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Direct Model Downloads\n",
        "\n",
        "from google.colab import userdata\n",
        "# get hugginface token\n",
        "userdata.get('HF_TOKEN')\n",
        "#get civitai token\n",
        "userdata.get('CIVIT_TOKEN')\n",
        "#OPTIONS = {}\n",
        "\n",
        "#USE_GOOGLE_DRIVE = True  #@param {type:\"boolean\"}\n",
        "WORKSPACE = 'ComfyUI'\n",
        "#OPTIONS['USE_GOOGLE_DRIVE'] = USE_GOOGLE_DRIVE\n",
        "\n",
        "#if OPTIONS['USE_GOOGLE_DRIVE']:\n",
        "if True:\n",
        "    !echo \"Mounting Google Drive...\"\n",
        "    %cd /\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    WORKSPACE = \"/content/drive/MyDrive/ComfyUI\"\n",
        "    %cd /content/drive/MyDrive/ComfyUI\n",
        "\n",
        "###install downloaders\n",
        "#civitai\n",
        "#!echo -= civitai =-\n",
        "#!pip install civitdl\n",
        "# Install huggingface model downloader\n",
        "#!echo -= huggingface_hub =-\n",
        "#!pip install huggingface_hub\n",
        "\n",
        "### 1.4\n",
        "## GLIGEN\n",
        "# 1.4 GLIGEN Textbox: Specialized model that enables spatial control of objects in generated images by specifying bounding boxes for text prompts, optimized with FP16 precision for reduced memory usage.\n",
        "!echo -= 1.4 GLIGEN Textbox =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/GLIGEN_pruned_safetensors/resolve/main/gligen_sd14_textbox_pruned_fp16.safetensors -O ./models/gligen/14_gligen_sd14_textbox_pruned_fp16.safetensors\n",
        "\n",
        "## T2I-Adapter\n",
        "# 1.4 T2I-Adapter Depth: Lightweight adapter (77M parameters) that provides depth-map conditioning for SD 1.4, using MiDaS depth estimation to guide image generation with spatial relationships.\n",
        "#!echo -= 1.4 T2I-Adapter Depth =-\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd14v1.pth -O ./models/controlnet/14_t2iadapter_depth_sd14v1.pth\n",
        "# 1.4 T2I-Adapter Segmentation: Lightweight adapter for SD 1.4 that enables semantic segmentation control, allowing region-specific content generation based on colored segment maps\n",
        "#!echo -= 1.4 T2I-Adapter Segmentation =-\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_seg_sd14v1.pth -O ./models/controlnet/14_t2iadapter_seg_sd14v1.pth\n",
        "# 1.4 T2I-Adapter Sketch: Lightweight adapter for SD 1.4 trained with PidiNet edge detection, enabling generation from hand-drawn monochrome sketches with white outlines on black backgrounds\n",
        "#!echo -= 1.4 T2I-Adapter Sketch =-\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_sketch_sd14v1.pth -O ./models/controlnet/14_t2iadapter_sketch_sd14v1.pth\n",
        "# 1.4 T2I-Adapter Keypose: Lightweight adapter for SD 1.4 trained with mmpose skeleton images, providing human pose control through skeleton-based conditioning\n",
        "#!echo -= 1.4 T2I-Adapter Keypose =-\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_keypose_sd14v1.pth -O ./models/controlnet/14_t2iadapter_keypose_sd14v1.pth\n",
        "# 1.4 T2I-Adapter OpenPose: Lightweight adapter for SD 1.4 that enables pose-based control using OpenPose bone images to guide human positioning and gestures\n",
        "#!echo -= 1.4 T2I-Adapter OpenPose =-\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_openpose_sd14v1.pth -O ./models/controlnet/14_t2iadapter_openpose_sd14v1.pth\n",
        "# 1.4 T2I-Adapter Color: Lightweight adapter for SD 1.4 trained with spatial color palettes, allowing precise control over the color scheme of generated images using 8x8 color grids\n",
        "#!echo -= 1.4 T2I-Adapter Color =-\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_color_sd14v1.pth -O ./models/controlnet/14_t2iadapter_color_sd14v1.pth\n",
        "# 1.4 T2I-Adapter Canny: Lightweight adapter for SD 1.4 trained with canny edge detection, guiding generation to follow edge outlines for precise shape control\n",
        "#!echo -= 1.4 T2I-Adapter Canny =-\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_canny_sd14v1.pth -O ./models/controlnet/14_t2iadapter_canny_sd14v1.pth\n",
        "\n",
        "## T2I Styles Model\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_style_sd14v1.pth -O ./models/style_models/\n",
        "\n",
        "## VAE\n",
        "# 1.4 Waifu Diffusion v1.4 VAE kl-f8-anime2: Specialized VAE for anime-style image generation, optimized for character-focused artwork and various anime aesthetics\n",
        "#!echo -= Waifu Diffusion v1.4 VAE kl-f8-anime2 =-\n",
        "#!wget -c https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt -P ./models/vae/14_kl-f8-anime2.ckpt\n",
        "\n",
        "\n",
        "### 1.5\n",
        "## CLIPVision model (needed for styles model)\n",
        "# 1.5 CLIP ViT-Large-Patch14: OpenAI's 428M parameter vision-language model using Vision Transformer architecture for zero-shot image classification, trained on diverse image-text pairs for robust visual understanding\n",
        "!echo -= 1.5 CLIP ViT-Large-Patch14 =-\n",
        "!wget -c https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/pytorch_model.bin -O ./models/clip_vision/15_clip_vit14.bin\n",
        "\n",
        "## ControlNet\n",
        "# 1.5 ControlNet IP2P: Half-precision model for instruct-pix2pix image editing, allowing text-based modifications to existing images\n",
        "!echo -= 1.5 ControlNet IP2P =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_ip2p_fp16.safetensors -O ./models/controlnet/15_control_v11e_sd15_ip2p_fp16.safetensors\n",
        "# 1.5 ControlNet Shuffle: Half-precision model that maintains content while allowing reorganization of image elements based on shuffled reference images\n",
        "!echo -= 1.5 ControlNet Shuffle =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_shuffle_fp16.safetensors -O ./models/controlnet/15_control_v11e_sd15_shuffle_fp16.safetensors\n",
        "# 1.5 ControlNet Canny: Half-precision edge detection model that guides generation following edge outlines, ideal for maintaining specific shapes\n",
        "!echo -= 1.5 ControlNet Canny =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_canny_fp16.safetensors -O ./models/controlnet/15_control_v11p_sd15_canny_fp16.safetensors\n",
        "# 1.5 ControlNet Depth: Half-precision depth map processor that maintains spatial relationships and perspective in generated images\n",
        "!echo -= 1.5 ControlNet Depth =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors -O ./models/controlnet/15_control_v11f1p_sd15_depth_fp16.safetensors\n",
        "# 1.5 ControlNet Inpaint: Half-precision model specialized for filling in masked areas while maintaining consistency with surrounding content\n",
        "!echo -= 1.5 ControlNet Inpaint =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_inpaint_fp16.safetensors -O ./models/controlnet/15_control_v11p_sd15_inpaint_fp16.safetensors\n",
        "# 1.5 ControlNet Lineart: Half-precision model for colorizing and enhancing line drawings while preserving original line structures\n",
        "!echo -= 1.5 ControlNet Lineart =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_lineart_fp16.safetensors -O ./models/controlnet/15_control_v11p_sd15_lineart_fp16.safetensors\n",
        "# 1.5 ControlNet MLSD: Half-precision model focused on detecting and preserving straight lines and geometric structures in images\n",
        "!echo -= .5 ControlNet MLSD =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_mlsd_fp16.safetensors -O ./models/controlnet/15_control_v11p_sd15_mlsd_fp16.safetensors\n",
        "# 1.5 ControlNet NormalBAE: Half-precision model using normal maps to control lighting and surface details in generated images\n",
        "!echo -= 1.5 ControlNet NormalBAE =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_normalbae_fp16.safetensors -O ./models/controlnet/15_control_v11p_sd15_normalbae_fp16.safetensors\n",
        "# 1.5 ControlNet OpenPose: Half-precision model that uses human pose estimation to control body positioning and gestures\n",
        "!echo -= 1.5 ControlNet OpenPose =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors -O ./models/controlnet/15_control_v11p_sd15_openpose_fp16.safetensors\n",
        "# 1.5 ControlNet Scribble: Half-precision model that generates images from simple sketches while maintaining the basic structure\n",
        "!echo -= 1.5 ControlNet Scribble =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors -O ./models/controlnet/15_control_v11p_sd15_scribble_fp16.safetensors\n",
        "# 1.5 ControlNet Segmentation: Half-precision model using semantic segmentation maps to control region-specific content generation\n",
        "!echo -= 1.5 ControlNet Segmentation =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_seg_fp16.safetensors -O ./models/controlnet/15_control_v11p_sd15_seg_fp16.safetensors\n",
        "# 1.5 ControlNet SoftEdge: Half-precision model using soft edge detection for more natural-looking guided generation\n",
        "!echo -= 1.5 ControlNet SoftEdge =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_softedge_fp16.safetensors -O ./models/controlnet/15_control_v11p_sd15_softedge_fp16.safetensors\n",
        "# 1.5 ControlNet Lineart Anime: Half-precision specialized model for anime-style line art colorization with improved results for cartoon aesthetics\n",
        "#!echo -= 1.5 ControlNet Lineart Anime =-\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15s2_lineart_anime_fp16.safetensors -O ./models/controlnet/15_control_v11p_sd15s2_lineart_anime_fp16.safetensors\n",
        "# 1.5 ControlNet Tile: Half-precision model enabling seamless texture generation and tiled outputs for creating repeating patterns\n",
        "!echo -= 1.5 ControlNet Tile =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11u_sd15_tile_fp16.safetensors -O ./models/controlnet/15_control_v11u_sd15_tile_fp16.safetensors\n",
        "\n",
        "## Loras\n",
        "# 1.5 Theovercomer8's Contrast Fix: LoRA for producing high-contrast, low-key images, trained on SD1.5 using offset noise technique.\n",
        "!echo -= 1.5 Theovercomer8s Contrast Fix =-\n",
        "!wget -c https://civitai.com/api/download/models/10638 -O ./models/loras/15_theovercomer8sContrastFix_sd15.safetensors\n",
        "\n",
        "## Models\n",
        "# 1.5 DreamShaper - A versatile SD 1.5-based model published in July 2023 with over 13,000 positive reviews. It's known for better handling of character LoRAs, improved photorealism, and better NSFW capabilities.\n",
        "!echo -= 1.5 Dreamshaper =-\n",
        "!wget -c https://civitai.com/models/4384/dreamshaper -O ./models/checkpoints/15_dreamshaper_8.safetensors\n",
        "\n",
        "# 1.5 Stable Diffusion v1.5 FP16 model: Archived version of the original SD 1.5, converted to FP16 precision with added metadata header, suitable for legacy testing and resource-constrained environments\n",
        "!echo -= 1.5 Stable Diffusion v1.5 FP16 model =-\n",
        "!wget -c https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/resolve/main/v1-5-pruned-emaonly-fp16.safetensors -o ./models/checkpoints/15_v1-5-pruned-emaonly-fp16.safetensors\n",
        "\n",
        "## unCLIP models\n",
        "# SD 1.5 illuminatiDiffusionV1 unCLIP: Fine-tuned model with unCLIP capabilities for reference-based image generation, optimized with half-precision (FP16) for reduced memory usage\n",
        "!echo -= SD 1.5 illuminatiDiffusionV1 unCLIP =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/illuminatiDiffusionV1_v11_unCLIP/resolve/main/illuminatiDiffusionV1_v11-unclip-h-fp16.safetensors -O ./models/checkpoints/15_illuminatiDiffusionV1_v11-unclip-h-fp16.safetensors\n",
        "# SD 1.5 WD 1.5 Beta2 Aesthetic unCLIP: Fine-tuned model with unCLIP capabilities for reference-based image generation, optimized with half-precision (FP16) for reduced memory usage.\n",
        "!echo -= SD 1.5 WD 1.5 Beta2 Aesthetic unCLIP =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/wd-1.5-beta2_unCLIP/resolve/main/wd-1-5-beta2-aesthetic-unclip-h-fp16.safetensors -O ./models/checkpoints/15_wd-1-5-beta2-aesthetic-unclip-h-fp16.safetensors\n",
        "\n",
        "## upscale model\n",
        "# 1.5 RealESRGAN x4plus: Powerful image upscaling model that enhances details while removing artifacts, optimized for 4x upscaling with improved performance on faces and real-world images\n",
        "!echo -= 1.5 RealESRGAN x4plus =-\n",
        "!wget -c https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -O ./models/upscale_models/15_RealESRGAN_x4plus.pth\n",
        "# 1.5 RealESRGAN x2: Image upscaling model optimized for 2x upscaling, providing a balance between detail enhancement and artifact removal for real-world images\n",
        "!echo -= 1.5 RealESRGAN x2 =-\n",
        "!wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x2.pth -O ./models/upscale_models/15_RealESRGAN_x2.pth\n",
        "# 1.5 RealESRGAN x4: Image upscaling model designed for 4x upscaling, offering detail enhancement and artifact removal capabilities similar to x4plus but potentially with different optimization parameters\n",
        "!echo -= 1.5 RealESRGAN x4 =-\n",
        "!wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x4.pth -O ./models/upscale_models/15_RealESRGAN_x4.pth\n",
        "\n",
        "## VAE\n",
        "# 1.5 SD VAE-FT-MSE: Improved autoencoder fine-tuned on LAION datasets with emphasis on MSE reconstruction, producing smoother outputs and better face reconstruction than the original SD 1.5 VAE\n",
        "!echo -= 1.5 SD VAE-FT-MSE =-\n",
        "!wget -c https://huggingface.co/stabilityai/sd-vae-ft-mse/resolve/main/diffusion_pytorch_model.safetensors -O ./models/vae/15_sd-vae-ft-mse.safetensors\n",
        "# 1.5 VAE-FT-MSE-840000: Improved autoencoder fine-tuned on LAION datasets for 840,000 steps, emphasizing MSE reconstruction for smoother outputs and better face reconstruction\n",
        "!echo -= 1.5 VAE-FT-MSE-840000 =-\n",
        "!wget -c https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors -O ./models/vae/15_vae-ft-mse-840000-ema-pruned.safetensors\n",
        "# 1.5 OrangeMix VAE: Specialized VAE for anime-style image generation, particularly designed for OrangeMix model family (AbyssOrangeMix, VividOrangeMix) to produce vibrant colors and proper skin tones\n",
        "#!echo -= 1.5 OrangeMix VAE =-\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/VAEs/orangemix.vae.pt -P ./models/vae/15_orangemix.vae.pt\n",
        "\n",
        "### 2.x\n",
        "## Loras\n",
        "# 2.1 768 Theovercomer8's Contrast Fix: LoRA for producing high-contrast, low-key images, trained on SD2.1-768 using offset noise technique.\n",
        "!echo -= 2.1 768 Theovercomer8s Contrast Fix =-\n",
        "!wget -c https://civitai.com/api/download/models/10350 -O ./models/loras/21_theovercomer8sContrastFix_sd21768.safetensors\n",
        "\n",
        "\n",
        "### SDXL\n",
        "## Loras\n",
        "# SDXL 1.0 Offset Example LoRA: Official LoRA for controlling brightness, contrast, and color in SDXL 1.0 generations using noise offset technique.\n",
        "!echo -= SDXL 1.0 Offset Example LoRA =-\n",
        "!wget -c https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_offset_example-lora_1.0.safetensors -O ./models/loras/sdxl_sd_xl_offset_example-lora_1.0.safetensors\n",
        "\n",
        "## Models\n",
        "# SDXL DreamShaper XL - The SDXL version published in February 2024 with nearly 5,000 positive reviews. The current version is based on SDXL Turbo and should be used with CFG scale 2 and 4-8 sampling steps, preferably with DPM++ SDE Karras sampler.\n",
        "!echo -= SDXL Dreamshaper XL =-\n",
        "!wget -c https://civitai.com/models/112902/dreamshaper-xl -O ./models/checkpoints/sdxl_dreamshaperXL_v21TurboDPMSDE.safetensors\n",
        "\n",
        "# SDXL ProteusV0.3: Advanced text-to-image model fine-tuned with 200,000+ anime images and DPO optimization, excelling at photorealistic and stylistic generation with enhanced facial details and lighting effects\n",
        "!echo -= SDXL ProteusV0.3 =-\n",
        "!wget -c https://huggingface.co/dataautogpt3/ProteusV0.3/resolve/main/sd_xl_base_1.0.safetensors -O ./models/checkpoints/sdxl_proteus_sd_xl_base_1.0.safetensors\n",
        "\n",
        "# SDXL Base 1.0: Stable Diffusion XL base model that generates initial latents with 3x larger UNet, dual text encoders, and improved capabilities for realistic faces, legible text, and better composition.\n",
        "!echo -= SDXL Base 1.0 =-\n",
        "!wget -c https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors -O ./models/checkpoints/sdxl_sd_xl_base_1.0.safetensors\n",
        "\n",
        "# SDXL Refiner 1.0: Specialized second-stage model that enhances outputs from the base model, implementing an ensemble of experts approach to improve visual fidelity and fix low-quality details like deformed faces.\n",
        "!echo -= SDXL Refiner 1.0 =-\n",
        "!wget -c https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/resolve/main/sd_xl_refiner_1.0.safetensors -O ./models/checkpoints/sdxl_sd_xl_refiner_1.0.safetensors\n",
        "\n",
        "## Clip Vision\n",
        "# SDXL CLIP Vision G: Large-scale vision encoder model (3.44GB) used in unCLIP workflows for visual encoding, enabling image-based conditioning and reference-guided generation in ComfyUI.\n",
        "!echo -= SDXL CLIP Vision G =-\n",
        "!wget -c https://huggingface.co/comfyanonymous/clip_vision_g/resolve/main/clip_vision_g.safetensors -O ./models/clip_vision/sdxl_clip_vision_g.safetensors\n",
        "\n",
        "## Control-lora\n",
        "# SDXL Control-LoRA Canny: Efficient edge-detection control model (738MB) that follows image edges for guided generation, significantly smaller than traditional 4.7GB ControlNets\n",
        "!echo -= SDXL Control-LoRA Canny =-\n",
        "!wget -c https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-canny-rank256.safetensors -O ./models/control-lora/sdxl_control-lora-canny-rank256.safetensors\n",
        "# SDXL Control-LoRA Depth: Depth-map control model using MiDaS and ClipDrop depth estimation to guide generation based on spatial relationships, optimized to 738MB for consumer GPUs\n",
        "!echo -= SDXL Control-LoRA Depth =-\n",
        "!wget -c https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-depth-rank256.safetensors -O ./models/control-lora/sdxl_control-lora-depth-rank256.safetensors\n",
        "# SDXL Control-LoRA Recolor: Specialized model for colorizing black and white photographs while maintaining high-quality control capabilities, compressed to 738MB through LoRA technology\n",
        "!echo -= Control-LoRA Recolor =-\n",
        "!wget -c https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-recolor-rank256.safetensors -O ./models/control-lora/sdxl_control-lora-recolor-rank256.safetensors\n",
        "# SDXL Control-LoRA Sketch: White-on-black drawing colorizer designed specifically for sketches and line art, reduced to 738MB from the original 4.7GB ControlNet size\n",
        "!echo -= Control-LoRA Sketch =-\n",
        "!wget -c https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-sketch-rank256.safetensors -O ./models/control-lora/sdxl_control-lora-sketch-rank256.safetensors\n",
        "\n",
        "\n",
        "### I recommend these workflow examples: https://comfyanonymous.github.io/ComfyUI_examples/sdxl/\n",
        "\n",
        "\n",
        "\n",
        "### IP-Adapter\n",
        "## IP-Adapter Models\n",
        "# IP-Adapter Models\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid-plus_sd15.bin -P ./models/ipadapter/\n",
        "# Basic model, average strength\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15.safetensors -P ./models/ipadapter/\n",
        "# Light impact model\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15_light_v11.bin -P ./models/ipadapter/\n",
        "# Plus model, very strong\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-plus_sd15.safetensors -P ./models/ipadapter/\n",
        "# Face model, portraits\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-plus-face_sd15.safetensors -P ./models/ipadapter/\n",
        "# Stronger face model, not necessarily better\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-full-face_sd15.safetensors -P ./models/ipadapter/\n",
        "# Base model, requires bigG clip vision encoder\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15_vit-G.safetensors -P ./models/ipadapter/\n",
        "# SDXL model\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter_sdxl_vit-h.safetensors -P ./models/ipadapter/\n",
        "# SDXL plus model\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter-plus_sdxl_vit-h.safetensors -P ./models/ipadapter/\n",
        "# SDXL face model\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter-plus-face_sdxl_vit-h.safetensors -P ./models/ipadapter/\n",
        "# vit-G SDXL model, requires bigG clip vision encoder\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter_sdxl.safetensors -P ./models/ipadapter/\n",
        "\n",
        "# IP-Adapter FaceID Models - FaceID models require insightface, you need to install it in your ComfyUI environment. Check this issue for help. Remember that most FaceID models also need a LoRA.\n",
        "# base FaceID model\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid_sd15.bin -P ./models/ipadapter/\n",
        "# FaceID plus v2\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid-plusv2_sd15.bin -P ./models/ipadapter/\n",
        "# text prompt style transfer for portraits\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid-portrait-v11_sd15.bin -P ./models/ipadapter/\n",
        "# SDXL base FaceID\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid_sdxl.bin -P ./models/ipadapter/\n",
        "# SDXL plus v2\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid-plusv2_sdxl.bin -P ./models/ipadapter/\n",
        "# SDXL text prompt style transfer\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid-portrait_sdxl.bin -P ./models/ipadapter/\n",
        "# very strong style transfer SDXL only\n",
        "!wget -c ip-adapter-faceid-portrait_sdxl_unnorm.bin -P ./models/ipadapter/\n",
        "# IP-Adapter - community - general composition ignoring style and content, more about it at https://huggingface.co/ostris/ip-composition-adapter\n",
        "!wget -c https://huggingface.co/ostris/ip-composition-adapter/resolve/main/ip_plus_composition_sd15.safetensors -P ./models/ipadapter/\n",
        "# SDXL version\n",
        "!wget -c https://huggingface.co/ostris/ip-composition-adapter/resolve/main/ip_plus_composition_sdxl.safetensors -P ./models/ipadapter/\n",
        "\n",
        "## IP-Adapter Loras\n",
        "# IP-Adapter - FaceID - Most FaceID models require a LoRA. If you use the IPAdapter Unified Loader FaceID it will be loaded automatically if you follow the naming convention. Otherwise you have to load them manually, be careful each FaceID model has to be paired with its own specific LoRA.\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid-plus_sd15_lora.safetensors -P ./models/loras/\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid-plusv2_sd15_lora.safetensors -P ./models/loras/\n",
        "# SDXL FaceID LoRA\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid_sdxl_lora.safetensors -P ./models/loras/\n",
        "# SDXL plus v2 LoRA\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid-plusv2_sdxl_lora.safetensors -P ./models/loras/\n",
        "\n",
        "## IP-Adapter CLIPVision models\n",
        "# 15 CLIP-ViT-H-14-laion2B-s32B-b79K: Large-scale vision-language model with ViT-H/14 architecture, trained on 2 billion image-text pairs for zero-shot classification and image-text retrieval tasks\n",
        "!echo -= 15 CLIP-ViT-H-14-laion2B-s32B-b79K =-\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter/resolve/main/models/image_encoder/model.safetensors -O ./models/clip_vision/15_CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors\n",
        "# SDXL CLIP-ViT-bigG-14-laion2B-39B-b160K: Advanced vision-language model with ViT-bigG/14 backbone, trained on 39 billion image-text pairs, achieving 80.1% zero-shot accuracy on ImageNet-1k\n",
        "!echo -= SDXL CLIP-ViT-bigG-14-laion2B-39B-b160K =-\n",
        "!wget -c https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/image_encoder/model.safetensors -O ./models/clip_vision/sdxl_CLIP-ViT-bigG-14-laion2B-39B-b160k.safetensors\n",
        "\n",
        "\n",
        "# SD2\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# Some SD1.5 anime style\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_hard.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A1_orangemixs.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A3_orangemixs.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/anything-v3-fp16-pruned.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# Waifu Diffusion 1.5 (anime style SD2.x 768-v)\n",
        "#!wget -c https://huggingface.co/waifu-diffusion/wd-1-5-beta3/resolve/main/wd-illusion-fp16.safetensors -P ./models/checkpoints/15_wd-1-5-beta3/resolve/main/wd-illusion-fp16.safetensors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Controlnet Preprocessor nodes by Fannovel16\n",
        "!cd custom_nodes && git clone https://github.com/Fannovel16/comfy_controlnet_preprocessors; cd comfy_controlnet_preprocessors && python install.py\n",
        "\n",
        "\n",
        "# Download and extract InsightFace buffalo_l models\n",
        "!echo -= Downloading InsightFace buffalo_l models =-\n",
        "!wget -c https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip -O /content/drive/MyDrive/ComfyUI/models/insightface/buffalo_l.zip\n",
        "!unzip -o /content/drive/MyDrive/ComfyUI/models/insightface/buffalo_l.zip -d /content/drive/MyDrive/ComfyUI/models/insightface/\n",
        "!rm /content/drive/MyDrive/ComfyUI/models/insightface/buffalo_l.zip\n",
        "\n",
        "\n",
        "# Download required SDXL LCM LoRA model (rename as required by Comfy_Dungeon)\n",
        "# SDXL LCM-LoRA: Latent Consistency Model adapter that dramatically reduces inference steps to 2-8 while maintaining quality, requiring guidance scale values between 1.0-2.0 for optimal results\n",
        "!echo -= SDXL LCM-LoRA for Comfy_Dungeon =-\n",
        "!wget -c https://huggingface.co/latent-consistency/lcm-lora-sdxl/resolve/main/pytorch_lora_weights.safetensors -O /content/drive/MyDrive/ComfyUI/models/loras/lcm-lora-sdxl.safetensors\n",
        "\n",
        "!echo -= Done =-\n"
      ],
      "metadata": {
        "id": "RxvUnGazeIcT",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gxv1ehd7VUU4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkkkkkkkkkkkkk"
      },
      "source": [
        "### Run ComfyUI with cloudflared (Recommended Way)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjjjjjjjjjjjjj"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import urllib.request\n",
        "\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  print(\"\\nComfyUI finished loading, trying to launch cloudflared (if it gets stuck here cloudflared is having issues)\\n\")\n",
        "\n",
        "  p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://127.0.0.1:{}\".format(port)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "  for line in p.stderr:\n",
        "    l = line.decode()\n",
        "    if \"trycloudflare.com \" in l:\n",
        "      print(\"This is the URL to access ComfyUI:\", l[l.find(\"http\"):], end='')\n",
        "    #print(l, end='')\n",
        "\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}